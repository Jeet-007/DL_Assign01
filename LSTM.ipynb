{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65534 65534 65534\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list()\n",
    "    for w in seq:\n",
    "        try:\n",
    "            idxs.append(to_ix[w])\n",
    "        except:\n",
    "            idxs.append(to_ix[\"unk\"])\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "full_dataset = pd.read_csv('/home/jeet/Downloads/work/SEM-VI/DL/Assignment/A1-Q3_Dataset-20190402T184338Z-001/A1-Q3_Dataset/mrdata.csv')\n",
    "\n",
    "# print(random.shuffle(full_dataset))\n",
    "training_data, testset = random_split(full_dataset, (int(len(full_dataset)*0.8), int(len(full_dataset))-int(len(full_dataset)*0.8)))\n",
    "\n",
    "# training_data = full_dataset[:int(len(full_dataset)*0.8)]\n",
    "# testset = full_dataset[int(len(full_dataset)*0.8):]\n",
    "training_data = training_data.dataset\n",
    "testset = testset.dataset\n",
    "print(len(full_dataset), len(training_data), len(testset))\n",
    "# training_data, testset = torch.utils.data.random_split(full_dataset, ((int(len(full_dataset)*0.8)), len(full_dataset) - int(len(full_dataset)*0.8)))\n",
    "# print(training_data.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = list()\n",
    "for phrase, tag in zip(training_data.Phrase, training_data.Sentiment):\n",
    "    trainset.append((str(phrase).split(), tag))\n",
    "# print(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = trainset\n",
    "\n",
    "tages = ['negative', 'somewhat negative', 'neutral', 'somewhat positive', 'positive']\n",
    "\n",
    "word_to_ix = {'unk':0}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "# print(word_to_ix)\n",
    "tag_to_ix = {'negative':0, 'somewhat negative':1, 'neutral':2, 'somewhat positive':3, 'positive':4}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModal(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMModal, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModal(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ....\n",
      "\n",
      "Epoch  0\n",
      "Counter  1000\n",
      "Counter  2000\n",
      "Counter  3000\n",
      "Counter  4000\n",
      "Counter  5000\n",
      "Counter  6000\n",
      "Counter  7000\n",
      "Counter  8000\n",
      "Counter  9000\n",
      "Counter  10000\n",
      "Counter  11000\n",
      "Counter  12000\n",
      "Counter  13000\n",
      "Counter  14000\n",
      "Counter  15000\n",
      "Counter  16000\n",
      "Counter  17000\n",
      "Counter  18000\n",
      "Counter  19000\n",
      "Counter  20000\n",
      "Counter  21000\n",
      "Counter  22000\n",
      "Counter  23000\n",
      "Counter  24000\n",
      "Counter  25000\n",
      "Counter  26000\n",
      "Counter  27000\n",
      "Counter  28000\n",
      "Counter  29000\n",
      "Counter  30000\n",
      "Counter  31000\n",
      "Counter  32000\n",
      "Counter  33000\n",
      "Counter  34000\n",
      "Counter  35000\n",
      "Counter  36000\n",
      "Counter  37000\n",
      "Counter  38000\n",
      "Counter  39000\n",
      "Counter  40000\n",
      "Counter  41000\n",
      "Counter  42000\n",
      "Counter  43000\n",
      "Counter  44000\n",
      "Counter  45000\n",
      "Counter  46000\n",
      "Counter  47000\n",
      "Counter  48000\n",
      "Counter  49000\n",
      "Counter  50000\n",
      "Counter  51000\n",
      "Counter  52000\n",
      "Counter  53000\n",
      "Counter  54000\n",
      "Counter  55000\n",
      "Counter  56000\n",
      "Counter  57000\n",
      "Counter  58000\n",
      "Counter  59000\n",
      "Counter  60000\n",
      "Counter  61000\n",
      "Counter  62000\n",
      "Counter  63000\n",
      "Counter  64000\n",
      "Counter  65000\n",
      "Epoch  1\n",
      "Counter  66000\n",
      "Counter  67000\n",
      "Counter  68000\n",
      "Counter  69000\n",
      "Counter  70000\n",
      "Counter  71000\n",
      "Counter  72000\n",
      "Counter  73000\n",
      "Counter  74000\n",
      "Counter  75000\n",
      "Counter  76000\n",
      "Counter  77000\n",
      "Counter  78000\n",
      "Counter  79000\n",
      "Counter  80000\n",
      "Counter  81000\n",
      "Counter  82000\n",
      "Counter  83000\n",
      "Counter  84000\n",
      "Counter  85000\n",
      "Counter  86000\n",
      "Counter  87000\n",
      "Counter  88000\n",
      "Counter  89000\n",
      "Counter  90000\n",
      "Counter  91000\n",
      "Counter  92000\n",
      "Counter  93000\n",
      "Counter  94000\n",
      "Counter  95000\n",
      "Counter  96000\n",
      "Counter  97000\n",
      "Counter  98000\n",
      "Counter  99000\n",
      "Counter  100000\n",
      "Counter  101000\n",
      "Counter  102000\n",
      "Counter  103000\n",
      "Counter  104000\n",
      "Counter  105000\n",
      "Counter  106000\n",
      "Counter  107000\n",
      "Counter  108000\n",
      "Counter  109000\n",
      "Counter  110000\n",
      "Counter  111000\n",
      "Counter  112000\n",
      "Counter  113000\n",
      "Counter  114000\n",
      "Counter  115000\n",
      "Counter  116000\n",
      "Counter  117000\n",
      "Counter  118000\n",
      "Counter  119000\n",
      "Counter  120000\n",
      "Counter  121000\n",
      "Counter  122000\n",
      "Counter  123000\n",
      "Counter  124000\n",
      "Counter  125000\n",
      "Counter  126000\n",
      "Counter  127000\n",
      "Counter  128000\n",
      "Counter  129000\n",
      "Counter  130000\n",
      "Counter  131000\n",
      "Epoch  2\n",
      "Counter  132000\n",
      "Counter  133000\n",
      "Counter  134000\n",
      "Counter  135000\n",
      "Counter  136000\n",
      "Counter  137000\n",
      "Counter  138000\n",
      "Counter  139000\n",
      "Counter  140000\n",
      "Counter  141000\n",
      "Counter  142000\n",
      "Counter  143000\n",
      "Counter  144000\n",
      "Counter  145000\n",
      "Counter  146000\n",
      "Counter  147000\n",
      "Counter  148000\n",
      "Counter  149000\n",
      "Counter  150000\n",
      "Counter  151000\n",
      "Counter  152000\n",
      "Counter  153000\n",
      "Counter  154000\n",
      "Counter  155000\n",
      "Counter  156000\n",
      "Counter  157000\n",
      "Counter  158000\n",
      "Counter  159000\n",
      "Counter  160000\n",
      "Counter  161000\n",
      "Counter  162000\n",
      "Counter  163000\n",
      "Counter  164000\n",
      "Counter  165000\n",
      "Counter  166000\n",
      "Counter  167000\n",
      "Counter  168000\n",
      "Counter  169000\n",
      "Counter  170000\n",
      "Counter  171000\n",
      "Counter  172000\n",
      "Counter  173000\n",
      "Counter  174000\n",
      "Counter  175000\n",
      "Counter  176000\n",
      "Counter  177000\n",
      "Counter  178000\n",
      "Counter  179000\n",
      "Counter  180000\n",
      "Counter  181000\n",
      "Counter  182000\n",
      "Counter  183000\n",
      "Counter  184000\n",
      "Counter  185000\n",
      "Counter  186000\n",
      "Counter  187000\n",
      "Counter  188000\n",
      "Counter  189000\n",
      "Counter  190000\n",
      "Counter  191000\n",
      "Counter  192000\n",
      "Counter  193000\n",
      "Counter  194000\n",
      "Counter  195000\n",
      "Counter  196000\n",
      "Epoch  3\n",
      "Counter  197000\n",
      "Counter  198000\n",
      "Counter  199000\n",
      "Counter  200000\n",
      "Counter  201000\n",
      "Counter  202000\n",
      "Counter  203000\n",
      "Counter  204000\n",
      "Counter  205000\n",
      "Counter  206000\n",
      "Counter  207000\n",
      "Counter  208000\n",
      "Counter  209000\n",
      "Counter  210000\n",
      "Counter  211000\n",
      "Counter  212000\n",
      "Counter  213000\n",
      "Counter  214000\n",
      "Counter  215000\n",
      "Counter  216000\n",
      "Counter  217000\n",
      "Counter  218000\n",
      "Counter  219000\n",
      "Counter  220000\n",
      "Counter  221000\n",
      "Counter  222000\n",
      "Counter  223000\n",
      "Counter  224000\n",
      "Counter  225000\n",
      "Counter  226000\n",
      "Counter  227000\n",
      "Counter  228000\n",
      "Counter  229000\n",
      "Counter  230000\n",
      "Counter  231000\n",
      "Counter  232000\n",
      "Counter  233000\n",
      "Counter  234000\n",
      "Counter  235000\n",
      "Counter  236000\n",
      "Counter  237000\n",
      "Counter  238000\n",
      "Counter  239000\n",
      "Counter  240000\n",
      "Counter  241000\n",
      "Counter  242000\n",
      "Counter  243000\n",
      "Counter  244000\n",
      "Counter  245000\n",
      "Counter  246000\n",
      "Counter  247000\n",
      "Counter  248000\n",
      "Counter  249000\n",
      "Counter  250000\n",
      "Counter  251000\n",
      "Counter  252000\n",
      "Counter  253000\n",
      "Counter  254000\n",
      "Counter  255000\n",
      "Counter  256000\n",
      "Counter  257000\n",
      "Counter  258000\n",
      "Counter  259000\n",
      "Counter  260000\n",
      "Counter  261000\n",
      "Counter  262000\n",
      "Epoch  4\n",
      "Counter  263000\n",
      "Counter  264000\n",
      "Counter  265000\n",
      "Counter  266000\n",
      "Counter  267000\n",
      "Counter  268000\n",
      "Counter  269000\n",
      "Counter  270000\n",
      "Counter  271000\n",
      "Counter  272000\n",
      "Counter  273000\n",
      "Counter  274000\n",
      "Counter  275000\n",
      "Counter  276000\n",
      "Counter  277000\n",
      "Counter  278000\n",
      "Counter  279000\n",
      "Counter  280000\n",
      "Counter  281000\n",
      "Counter  282000\n",
      "Counter  283000\n",
      "Counter  284000\n",
      "Counter  285000\n",
      "Counter  286000\n",
      "Counter  287000\n",
      "Counter  288000\n",
      "Counter  289000\n",
      "Counter  290000\n",
      "Counter  291000\n",
      "Counter  292000\n",
      "Counter  293000\n",
      "Counter  294000\n",
      "Counter  295000\n",
      "Counter  296000\n",
      "Counter  297000\n",
      "Counter  298000\n",
      "Counter  299000\n",
      "Counter  300000\n",
      "Counter  301000\n",
      "Counter  302000\n",
      "Counter  303000\n",
      "Counter  304000\n",
      "Counter  305000\n",
      "Counter  306000\n",
      "Counter  307000\n",
      "Counter  308000\n",
      "Counter  309000\n",
      "Counter  310000\n",
      "Counter  311000\n",
      "Counter  312000\n",
      "Counter  313000\n",
      "Counter  314000\n",
      "Counter  315000\n",
      "Counter  316000\n",
      "Counter  317000\n",
      "Counter  318000\n",
      "Counter  319000\n",
      "Counter  320000\n",
      "Counter  321000\n",
      "Counter  322000\n",
      "Counter  323000\n",
      "Counter  324000\n",
      "Counter  325000\n",
      "Counter  326000\n",
      "Counter  327000\n",
      "Training Done....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter=0\n",
    "print(\"Training ....\\n\")\n",
    "for epoch in range(5):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for sentence, tags in training_data:\n",
    "#         print(sentence, tags)\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#         print(sentence_in)\n",
    "        targets = tags\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "        tag_scores = torch.sum(tag_scores, dim=0)/len(sentence_in)\n",
    "#         print(torch.sum(tag_scores, dim=0))\n",
    "#         print(targets)\n",
    "#         print(tag_scores)\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores.unsqueeze(0), torch.tensor(targets).unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter+=1\n",
    "        if counter%1000==0:\n",
    "            print(\"Counter \", counter)\n",
    "\n",
    "print(\"Training Done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ....\n",
      "\n",
      "Accuracy : 56.21814630573443\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "total = 0\n",
    "correct = 0\n",
    "print(\"Testing ....\\n\")\n",
    "with torch.no_grad():\n",
    "    for phrase, tag in zip(testset.Phrase, testset.Sentiment):\n",
    "#         print(phrase, tag)\n",
    "        inputs = prepare_sequence(str(phrase).split(), word_to_ix)\n",
    "#         print(phrase, inputs)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        tag_scores = torch.sum(tag_scores, dim=0)/len(inputs)\n",
    "        values, indices = torch.max(tag_scores, 0)\n",
    "        \n",
    "#         print(tag_scores)\n",
    "        if indices.numpy() == tag:\n",
    "            correct+=1\n",
    "\n",
    "        #     print(values, indices.numpy(), training_data[0][1])\n",
    "        total+=1\n",
    "        \n",
    "accuracy = (correct*100)/(total*1.0)\n",
    "print(\"Accuracy :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
